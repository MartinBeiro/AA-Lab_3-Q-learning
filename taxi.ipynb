{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la biblioteca Gymnassium, que vamos a usar como framework de RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install cmake gymnasium scipy\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un ambiente y lo mostramos en pantalla. Para esto definimos una función para imprimir nuestro ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La semilla usada para crear el ambiente\n",
    "semilla = 1\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "# Una funcion de ayuda para imprimir el estado de nuestro mundo\n",
    "def print_env(estado):\n",
    "  env_str = estado.render()\n",
    "  print(env_str)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El rectángulo de color representa el taxi, amarillo cuando va sin pasajero y verde con pasajero.\n",
    "'|' representa una pared que el taxi no puede cruzar, es decir.\n",
    "R, G, Y, B son los puntos de interés, es decir, las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida de pasajeros, y la letra púrpura es el destino actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si cambiamos la semilla, cambia el estado del ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una semilla diferente\n",
    "semilla = 2\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos el espacio de estados y de acciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Espacio de Acciones {entorno.action_space}\")\n",
    "print(f\"Espacio de Estados {entorno.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 6 acciones, que corresponden a:\n",
    " * 0 = ir al Sur\n",
    " * 1 = ir al Norte\n",
    " * 2 = ir al Este\n",
    " * 3 = ir al Oeste\n",
    " * 4 = recoger pasajero\n",
    " * 5 = dejar pasajero\n",
    "\n",
    "Los puntos cardinales siguen la convención Norte hacie arriba. Recoger/dejar al pasajero solo tienen efecto si el taxi está en la misma casilla que el pasajero, y en uno de los puntos de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro agente deberá elegir la acción a tomar en cada paso. Gymnassium nos expone funciones para esto. Si queremos movernos al sur, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "print_env(entorno)\n",
    "\n",
    "accion = 0 # Sur\n",
    "entorno.step(accion)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estamos listos para programar un agente. Empezando por uno random. Se puede ejecutar el codigo abajo varias veces para ver como cambia en cada ejecución, debido a que la semilla_acciones es diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def episodio_random(semilla_ambiente = 1):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "    entorno.reset(seed = semilla_ambiente)\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # para la animación\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "\n",
    "    while not termino and not truncado:\n",
    "        #  selecciona una acción aleatoria del conjunto de todas las posibles acciones\n",
    "        accion = entorno.action_space.sample() \n",
    "        estado, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        iteraciones += 1\n",
    "\n",
    "\n",
    "    print(f\"Iteraciones: {iteraciones}\")\n",
    "    print(f\"Penalizaciones: {penalizaciones}\")\n",
    "\n",
    "    return marcos\n",
    "\n",
    "marcos = episodio_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el episodio completo abajo. Notar que seleccionamos la semillia de selector de acciones para que la corrida sea 'buena'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "def print_frames(marcos):\n",
    "    for i, marco in enumerate(marcos):\n",
    "        clear_output()\n",
    "        print(marco['marco'])\n",
    "        print(f\"Iteracion: {i + 1}\")\n",
    "        print(f\"Estado: {marco['estado']}\")\n",
    "        print(f\"Accion: {marco['accion']}\")\n",
    "        print(f\"Recompensa: {marco['recompensa']}\")\n",
    "        sys.stdout.flush()\n",
    "        # Aumentar este tiempo para ver mejor la animación\n",
    "        sleep(.01)\n",
    "\n",
    "print_frames(marcos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queremos programar un agente inteligente, para eso nos vamos a atener a la siguiente interfaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro agente aleatorio, esto sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # No aprende\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniendolo a jugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "agente = AgenteAleatorio()\n",
    "\n",
    "iteraciones = 0\n",
    "penalizaciones, recompensa = 0, 0\n",
    "\n",
    "marcos = [] # for animation\n",
    "\n",
    "termino = False\n",
    "truncado = False\n",
    "estado_anterior, info = entorno.reset(seed = semilla)\n",
    "while not termino and not truncado:\n",
    "    # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "    accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "    # Realizamos la accion\n",
    "    estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "    # Le informamos al agente para que aprenda\n",
    "    agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "    # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "    if recompensa == -10:\n",
    "        penalizaciones += 1\n",
    "\n",
    "    # Put each rendered frame into dict for animation\n",
    "    marcos.append({\n",
    "        'marco': entorno.render(),\n",
    "        'estado': estado_siguiente,\n",
    "        'accion': accion,\n",
    "        'recompensa': recompensa\n",
    "        }\n",
    "    )\n",
    "\n",
    "    estado_anterior = estado_siguiente\n",
    "    iteraciones += 1\n",
    "\n",
    "\n",
    "print(f\"Iteraciones: {iteraciones}\")\n",
    "print(f\"Penalizaciones: {penalizaciones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos encapsular lo anterior en una función "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, semilla):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # for animation\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed = semilla)\n",
    "    while not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado_siguiente,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "    return iteraciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y correrlo varias veces para ver el rendimiento promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteAleatorio()\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "for i in range(10):\n",
    "    num_iteraciones = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y obtener métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(f\"Se realizaron {numpy.mean(num_iteraciones_episodios)}, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class AgenteRL(Agente):\n",
    "    # Agregar código aqui\n",
    "\n",
    "    def __init__(self, entorno) -> None:\n",
    "        super().__init__()\n",
    "        # Agregar código aqui\n",
    "        ## hay que inizializar en random\n",
    "\n",
    "        self.q_table= np.zeros([entorno.observation_space.n, entorno.action_space.n])\n",
    "        self.gama=0.9\n",
    "        self.epsilon=0.2\n",
    "        self.alfa=0.7\n",
    "        self.k=1\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        ##aca hay que agregar la aleatoriedad?\n",
    "        if random.uniform(0,1)<self.epsilon:\n",
    "            return random.randrange(max_accion)\n",
    "        else:\n",
    "            prox_accion=np.argmax(self.q_table[estado])\n",
    "        return prox_accion\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # Agregar código aqui\n",
    "        self.q_table[estado_anterior][accion]=(1-self.alfa)*self.q_table[estado_anterior][accion]+self.alfa*(recompensa+self.gama*np.max(self.q_table[estado_siguiente]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios con la misma semilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "print(entorno.observation_space.n)\n",
    "agente = AgenteRL(entorno)\n",
    "semilla = 1\n",
    "num_ceros_matriz = []\n",
    "num_iteraciones_episodios = []\n",
    "for i in range(1000):\n",
    "    num_iteraciones = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_ceros_matriz += [np.size(agente.q_table)-np.count_nonzero(agente.q_table)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizar los resultados de la ejecución anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar código aqui\n",
    "\n",
    "#sns.lineplot(num_iteraciones_episodios)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(num_iteraciones_episodios)\n",
    "plt.xlabel(\"Iteración de entrenamiento\")\n",
    "plt.ylabel(\"Cantidad de iteraciones en resolver el problema\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gráfico podemos observar como el agente comienza a entrenarse logrando completar la tarea en menor cantidad de iteraciones conforme atraviesa un nuevo entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(f\"Se realizaron {numpy.mean(num_iteraciones_episodios)}, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio baja  mucho con especto al agente aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio_sin_entrenar(agente, semilla):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # for animation\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed = semilla)\n",
    "    while not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado_siguiente,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        if (iteraciones>100000):\n",
    "            break\n",
    "    #print_frames(marcos)\n",
    "\n",
    "    return iteraciones\n",
    "\n",
    "\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "# for i in range(1):\n",
    "num_iteraciones = ejecutar_episodio_sin_entrenar(agente, semilla)\n",
    "num_iteraciones_episodios += [num_iteraciones]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mantiene el rendimiento si cambiamos la semilla? Por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "semilla = 2\n",
    "num_iteraciones_episodios = []\n",
    "for i in range(10):\n",
    "    num_iteraciones = ejecutar_episodio_sin_entrenar(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    \n",
    "plt.plot(num_iteraciones_episodios)\n",
    "plt.xlabel(\"Iteración de prueba\")\n",
    "plt.ylabel(\"Cantidad de iteraciones en resolver el problema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si probamos el desempeño del agente frente a una nueva semilla, e impidiendo que aprenda, se observa como en la mayoría de los casos se alcanza el límite de iteraciones, lo que indica que no es capaz de resolver el problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'La matriz tiene {num_ceros_matriz[-1]} ceros de un total de  {np.size(agente.q_table)} entradas')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al hallar la cantidad de ceros en la matriz del agente, se observa como gran parte de la matriz (2400 de 3000 entradas) no tiene información. Esto significa que para muchos estados el agente no tiene una política definida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos mejorar el agente para que se desempeñe bien usando cualquier semilla?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente podemos entrenar el agente con más de una semilla y evaluar su desempeño.\n",
    "Para el siguiente caso se entrenará utilizando las semillas 1 y 2. Adicionalmente se graficará el número de iteraciones que le lleve al agente en completar la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Agregar código aqui\n",
    "agente = AgenteRL(entorno)\n",
    "semillas = range(1,3)\n",
    "num_iteraciones_episodios = []\n",
    "num_ceros_matriz = []\n",
    "for semilla in semillas:\n",
    "    for i in range(50):        \n",
    "        num_iteraciones = ejecutar_episodio(agente, semilla)\n",
    "        num_iteraciones_episodios += [num_iteraciones]\n",
    "        num_ceros_matriz += [np.size(agente.q_table)-np.count_nonzero(agente.q_table)]\n",
    "\n",
    "plt.plot(num_iteraciones_episodios)\n",
    "plt.xlabel(\"Iteración de entrenamiento\")\n",
    "plt.ylabel(\"Cantidad de iteraciones en resolver el problema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'La matriz tiene {np.size(agente.q_table)-np.count_nonzero(agente.q_table)} ceros de un total de  {np.size(agente.q_table)} entradas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gráfico se observa como en las 50 primeras iteraciones se utiliza la semilla 1, logrando que el agente complete la tarea en cada vez menos iteraciones. Las siguientes 50 iteraciones se cambia la semilla, por lo que se observa un desmejoramiento del desempeño, el cual vuelve a mejorar rápidamente. La cantidad de ceros en la matriz permanace incambiada a pesar de que ha expandido su política de decisión.\n",
    "Resta comprobar si al entrenarse por úlitmo para la semilla 2 es capaz que resolver el problema con la semilla 1 sin volver a entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "for i in range(1000):\n",
    "    num_iteraciones = ejecutar_episodio_sin_entrenar(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    \n",
    "plt.plot(num_iteraciones_episodios)\n",
    "plt.xlabel(\"Iteración de prueba\")\n",
    "plt.ylabel(\"Cantidad de iteraciones en resolver el problema\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prom_sin_fallas=np.mean(np.array(num_iteraciones_episodios)[np.array(num_iteraciones_episodios)<100000])\n",
    "print(f'En los casos de éxito, se realizaron {prom_sin_fallas} iteraciones, en promedio.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente pudo resolver el problema de manera correcta y en una cantidad razonable de iteraciones en la amplia mayoría de los casos.\n",
    "En esta instancia será capaz que resolver problemas con semillas que inician el problema de forma similar que la 1 y la 2. A modo de ejemplo, se prueba con la semilla 9, con la cual nunca ha sido entrenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "semilla = 9\n",
    "num_iteraciones_episodios = []\n",
    "for i in range(1000):\n",
    "    num_iteraciones = ejecutar_episodio_sin_entrenar(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    \n",
    "plt.plot(num_iteraciones_episodios)\n",
    "plt.xlabel(\"Iteración de prueba\")\n",
    "plt.ylabel(\"Cantidad de iteraciones en resolver el problema\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prom_sin_fallas=np.mean(np.array(num_iteraciones_episodios)[np.array(num_iteraciones_episodios)<100000])\n",
    "print(f'En los casos de éxito, se realizaron {prom_sin_fallas} iteraciones, en promedio.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos mejorar el agente para que se desempeñe bien usando cualquier semilla?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso anterior, fue capaz que resolver el problema aunque en más iteraciones comparado con la semilla 1.\n",
    "Por lo tanto, es posible que, si se entrenara con una cantidad suficiente de semillas, el agente pueda dejar al pasajero en destino para una semilla aleatoria.\n",
    "Para lo siguiente se entrenará con semillas del 1 al 99 corriendo 50 iteraciones por semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agente = AgenteRL(entorno)\n",
    "semillas = range(1,100)\n",
    "num_iteraciones_episodios = []\n",
    "num_ceros_matriz = []\n",
    "for semilla in semillas:\n",
    "    for i in range(50):        \n",
    "        num_iteraciones = ejecutar_episodio(agente, semilla)\n",
    "        num_iteraciones_episodios += [num_iteraciones]\n",
    "        num_ceros_matriz += [np.size(agente.q_table)-np.count_nonzero(agente.q_table)]\n",
    "\n",
    "plt.plot(num_iteraciones_episodios)\n",
    "plt.xlabel(\"Iteración de entrenamiento\")\n",
    "plt.ylabel(\"Cantidad de iteraciones en resolver el problema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico muestra como a medida que vamos cambiando las semillas, el agente se entrena cada vez más rápido. Incluso llega a un punto que, con semillas nuevas, su desempeño inicial es bueno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(num_ceros_matriz)\n",
    "plt.xlabel(\"Iteración de entrenamiento\")\n",
    "plt.ylabel(\"Cantidad de ceros en la matriz\")\n",
    "print(f'La matriz tiene {num_ceros_matriz[-1]} ceros de un total de  {np.size(agente.q_table)} entradas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se observa la cantidad de ceros de la matriz, ya ha llegado a su límite, ya que solo 400 estados puden ser alcanzados en un episodio para entrenar. Los restantes corresponden a situaciones donde el pasajero llega o aparece justo donde debe bajar, y la tarea ya se da como completada. Por lo que 100 estados por 6 acciones se obtiene la cantidad de ceros límite de entrenamiento.\n",
    "A continuación se puede probar el desempño utilizando semillas aleatorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "num_iteraciones_episodios = []\n",
    "for i in range(1000):\n",
    "    semilla = np.random.randint(100,10000)\n",
    "    num_iteraciones = ejecutar_episodio_sin_entrenar(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    \n",
    "plt.plot(num_iteraciones_episodios)\n",
    "plt.xlabel(\"Iteración de prueba\")\n",
    "plt.ylabel(\"Cantidad de iteraciones en resolver el problema\")\n",
    "\n",
    "print(f\"Se realizaron {numpy.mean(num_iteraciones_episodios)}, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de haberle pasado 1000 semillas aleatrias diferentes a las usadas para el entrenamiento, se observa como el agente es capaz que resolver todos los problemas.\n",
    "Incluso se podría mejorar el tiempo de entrenamiento poniedo un criterio de parada para cada semilla si logra resolver el problema en menos de uan cantidad arbitraria de iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Agregar código aqui\n",
    "limite_entrenamiento = 20\n",
    "agente = AgenteRL(entorno)\n",
    "semillas = range(1,100)\n",
    "num_iteraciones_episodios = []\n",
    "arranque_episodios = []\n",
    "iteraciones_arranques = []\n",
    "num_ceros_matriz = []\n",
    "for semilla in semillas:\n",
    "    for i in range(50):        \n",
    "        num_iteraciones = ejecutar_episodio(agente, semilla)\n",
    "        num_iteraciones_episodios += [num_iteraciones]\n",
    "        num_ceros_matriz += [np.size(agente.q_table)-np.count_nonzero(agente.q_table)]\n",
    "        if i ==0:\n",
    "            arranque_episodios += [len(num_iteraciones_episodios)-1]\n",
    "            iteraciones_arranques += [num_iteraciones_episodios[-1]]\n",
    "        if num_iteraciones<limite_entrenamiento:\n",
    "            break\n",
    "\n",
    "plt.plot(num_iteraciones_episodios)\n",
    "plt.plot(arranque_episodios,iteraciones_arranques, '.r', label=\"Semilla nueva\")\n",
    "plt.xlabel(\"Iteración de entrenamiento\")\n",
    "plt.ylabel(\"Cantidad de iteraciones en resolver el problema\")\n",
    "plt.legend()\n",
    "#plt.xlim([98,102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los puntos rojos representan el inicio de cada entrenamiento con una nueva semilla. La cantidad total de iteraciones usadas con todas las semillas se reduce un orden de magnitud utilizando el criterio mencionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_iteraciones_episodios = []\n",
    "for i in range(1000):\n",
    "    semilla = np.random.randint(100,10000)\n",
    "    num_iteraciones = ejecutar_episodio_sin_entrenar(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    \n",
    "plt.plot(num_iteraciones_episodios)\n",
    "plt.xlabel(\"Iteración de prueba\")\n",
    "plt.ylabel(\"Cantidad de iteraciones en resolver el problema\")\n",
    "\n",
    "print(f\"Se realizaron {numpy.mean(num_iteraciones_episodios)}, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando nuevamente semillas aleatorias se observa como el agente puede resolver el problema, aunque en una cantidad mayor de iteraciones en promedio. Hay un compromiso entre tiempo de entrenamiento y desempeño al elegir un criterio de parada de entrenamiento para cada semilla."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
